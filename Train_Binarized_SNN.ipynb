{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuromorphs/osn23-huge-tapeout/blob/main/Train_Binarized_SNN.ipynb)"
      ],
      "metadata": {
        "id": "w5wJQOqFjbwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zadn76o6NGl4",
        "outputId": "9db4bd76-5a24-4569-c7c4-3c62daf320ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snntorch in /usr/local/lib/python3.10/dist-packages (0.6.4)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->snntorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->snntorch) (16.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "config = {\n",
        "        'model' : 'NetFC',\n",
        "\n",
        "        'exp_name' : 'mnist_tha',\n",
        "        'num_trials' : 5,\n",
        "        'num_epochs' : 10, #500,\n",
        "        'binarize' : True,\n",
        "        'binarize_input' : True,\n",
        "        'data_dir' : \"~/data/mnist\",\n",
        "        'batch_size' : 128,\n",
        "        'seed' : 0,\n",
        "        'num_workers' : 0,\n",
        "\n",
        "        # final run sweeps\n",
        "        'save_csv' : True,\n",
        "        'save_model' : True,\n",
        "        'early_stopping': True,\n",
        "        'patience': 100,\n",
        "\n",
        "        # final params\n",
        "        'grad_clip' : False,\n",
        "        'weight_clip' : False,\n",
        "        'batch_norm' : True,\n",
        "        'dropout1' : 0.02856,\n",
        "        'beta' : 0.99,\n",
        "        'lr' : 9.97e-3,\n",
        "        'slope': 10.22,\n",
        "\n",
        "        # threshold annealing. note: thr_final = threshold + thr_final\n",
        "        'threshold1' : 11.666,\n",
        "        'alpha_thr1' : 0.024,\n",
        "        'thr_final1' : 4.317,\n",
        "\n",
        "        'threshold2' : 14.105,\n",
        "        'alpha_thr2' : 0.119,\n",
        "        'thr_final2' : 16.29,\n",
        "\n",
        "        'threshold3' : 0.6656,\n",
        "        'alpha_thr3' : 0.0011,\n",
        "        'thr_final3' : 3.496,\n",
        "\n",
        "        # fixed params\n",
        "        'num_steps' : 100,\n",
        "        'correct_rate': 0.8,\n",
        "        'incorrect_rate' : 0.2,\n",
        "        'betas' : (0.9, 0.999),\n",
        "        't_0' : 4688,\n",
        "        'eta_min' : 0,\n",
        "        'df_lr' : True, # return learning rate. Useful for scheduling\n",
        "    }\n",
        "\n",
        "def optim_func(net, config):\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=config[\"lr\"], betas=config['betas'])\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['t_0'], eta_min=config['eta_min'], last_epoch=-1)\n",
        "    return optimizer, scheduler\n"
      ],
      "metadata": {
        "id": "O0L13aqxMYtN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "\n",
        "class BinarizeF(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = input.new(input.size())\n",
        "        output[input >= 0] = 1\n",
        "        output[input < 0] = -1\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        grad_input = grad_output.clone()\n",
        "        return grad_input\n",
        "\n",
        "# aliases\n",
        "binarize = BinarizeF.apply\n",
        "\n",
        "def binarize_activations(input):\n",
        "    output = input.new(input.size())\n",
        "    output[input >= 0.5] = 1\n",
        "    output[input < 0.5] = 0\n",
        "    return output"
      ],
      "metadata": {
        "id": "vb6YCUbWMfG6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wED_xSVyMTfi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class BinaryTanh(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryTanh, self).__init__()\n",
        "        self.hardtanh = nn.Hardtanh()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.hardtanh(input)\n",
        "        output = binarize(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class BinaryLinear(nn.Linear):\n",
        "    def forward(self, input):\n",
        "        binary_weight = binarize(self.weight)\n",
        "        if self.bias is None:\n",
        "            return F.linear(input, binary_weight)\n",
        "        else:\n",
        "            return F.linear(input, binary_weight, self.bias)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Glorot initialization\n",
        "        in_features, out_features = self.weight.size()\n",
        "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.zero_()\n",
        "\n",
        "        self.weight.lr_scale = 1. / stdv\n",
        "\n",
        "class SparseBinaryLinear(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, sparsity=0, bias=True, device=None, dtype=None):\n",
        "        super(SparseBinaryLinear, self).__init__(in_features, out_features, bias, device, dtype)\n",
        "        self.mask = torch.bernoulli(torch.ones_like(self.weight) * (1-sparsity))\n",
        "        self.register_buffer('weight_mask_const', self.mask)\n",
        "        print(self.mask.mean())\n",
        "\n",
        "    def forward(self, input):\n",
        "        binary_weight = binarize(self.weight).mul(Variable(self.weight_mask_const))\n",
        "        #print(self.weight_mask_const.mean(), binary_weight.mean())\n",
        "        if self.bias is None:\n",
        "            return F.linear(input, binary_weight)\n",
        "        else:\n",
        "            return F.linear(input, binary_weight, self.bias)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Glorot initialization\n",
        "        in_features, out_features = self.weight.size()\n",
        "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.zero_()\n",
        "\n",
        "        self.weight.lr_scale = 1. / stdv\n",
        "\n",
        "class BinaryConv2d(nn.Conv2d):\n",
        "\n",
        "    def forward(self, input):\n",
        "        bw = binarize(self.weight)\n",
        "        return F.conv2d(input, bw, self.bias, self.stride,\n",
        "                               self.padding, self.dilation, self.groups)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Glorot initialization\n",
        "        in_features = self.in_channels\n",
        "        out_features = self.out_channels\n",
        "        for k in self.kernel_size:\n",
        "            in_features *= k\n",
        "            out_features *= k\n",
        "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.zero_()\n",
        "\n",
        "        self.weight.lr_scale = 1. / stdv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "MNIST_INPUT_RESOLUTION = 16\n",
        "\n",
        "def load_data(config):\n",
        "        data_dir = config['data_dir']\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "                transforms.Resize((MNIST_INPUT_RESOLUTION, MNIST_INPUT_RESOLUTION)),\n",
        "                transforms.Grayscale(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0,), (1,))])\n",
        "\n",
        "        trainset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\n",
        "        testset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "        return trainset, testset"
      ],
      "metadata": {
        "id": "TPJW3wqPMbEX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class EarlyStopping_acc:\n",
        "    \"\"\"Early stops the training if test acc doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.test_loss_min = 0\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, test_loss, model):\n",
        "\n",
        "        score = test_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(test_loss, model)\n",
        "        elif score <= self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                self.counter = 0\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(test_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, test_loss, model):\n",
        "        '''Saves model when test acc increases.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Test acc increased ({self.test_loss_min:.6f} --> {test_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.test_loss_min = test_loss"
      ],
      "metadata": {
        "id": "K4S2cHioMdrY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import snntorch as snn\n",
        "from snntorch import functional as SF\n",
        "\n",
        "def test_accuracy(config, net, testloader, device=\"cpu\"):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs, _ = net(images)\n",
        "            accuracy = SF.accuracy_rate(outputs, labels)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += accuracy * labels.size(0)\n",
        "\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "VXtq5vi-MlL6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exp relaxation implementation of THA based on Eq (4)\n",
        "\n",
        "def thr_annealing(config, network):\n",
        "    alpha_thr1 = config['alpha_thr1']\n",
        "    alpha_thr2 = config['alpha_thr2']\n",
        "    alpha_thr3 = config['alpha_thr3']\n",
        "\n",
        "    ### to address conditional parameters, s.t. thr_final > threshold\n",
        "    thr_final1 = config['thr_final1'] + config['threshold1']\n",
        "    thr_final2 = config['thr_final2'] + config['threshold2']\n",
        "    thr_final3 = config['thr_final3'] + config['threshold3']\n",
        "\n",
        "    network.lif1.threshold += (thr_final1 - network.lif1.threshold) * alpha_thr1\n",
        "    network.lif2.threshold += (thr_final2 - network.lif2.threshold) * alpha_thr2\n",
        "    network.lif3.threshold += (thr_final3 - network.lif3.threshold) * alpha_thr3\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "DOU3fLVkMmJO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# snntorch\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# misc\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(config, net, epoch, trainloader, testloader, criterion, optimizer, scheduler, device):\n",
        "    net.train()\n",
        "    loss_accum = []\n",
        "    lr_accum = []\n",
        "\n",
        "    # TRAIN\n",
        "    progress_bar = tqdm(trainloader)\n",
        "    loss_current = None\n",
        "\n",
        "    #for data, labels in trainloader:\n",
        "    for data, labels in progress_bar:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "        spk_rec2, _ = net(data)\n",
        "        loss = criterion(spk_rec2, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if loss_current is None:\n",
        "            loss_current = loss.item()\n",
        "        else:\n",
        "            loss_current = 0.9 * loss_current + 0.1 * loss.item()\n",
        "        progress_bar.set_description(f\"loss: {loss_current:.4f}\")\n",
        "\n",
        "        if config['grad_clip']:\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "        if config['weight_clip']:\n",
        "            with torch.no_grad():\n",
        "                for param in net.parameters():\n",
        "                    param.clamp_(-1, 1)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        thr_annealing(config, net)\n",
        "\n",
        "\n",
        "        loss_accum.append(loss.item()/config['num_steps'])\n",
        "        lr_accum.append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "\n",
        "    return loss_accum, lr_accum"
      ],
      "metadata": {
        "id": "VD1SnCZgMo2m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# snntorch\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "from snntorch import surrogate\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# misc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "\n",
        "def run(config):\n",
        "    print(config)\n",
        "    file_name = config['exp_name']\n",
        "\n",
        "    for trial in range(config['num_trials']):\n",
        "        # file names\n",
        "        SAVE_CSV = config['save_csv']\n",
        "        SAVE_MODEL = config['save_model']\n",
        "        csv_name = file_name + '_t' + str(trial) + '.csv'\n",
        "        log_name = file_name + '_t' + str(trial) + '.log'\n",
        "        model_name = file_name + '_t' + str(trial) + '.pt'\n",
        "        num_epochs = config['num_epochs']\n",
        "        torch.manual_seed(config['seed'])\n",
        "\n",
        "        # dataframes\n",
        "        df_train_loss = pd.DataFrame()\n",
        "        df_test_acc = pd.DataFrame(columns=['epoch', 'test_acc', 'train_time'])\n",
        "        df_lr = pd.DataFrame()\n",
        "\n",
        "        # initialize network\n",
        "        net = None\n",
        "        net_desc = config['model']\n",
        "        if net_desc in globals():\n",
        "            klass = globals()[net_desc]\n",
        "            net = klass(config)\n",
        "        else:\n",
        "            net = eval(net_desc)\n",
        "        if trial == 0:\n",
        "            print(net)\n",
        "        device = \"cpu\"\n",
        "        #device = \"mps\"\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda:0\"\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                net = nn.DataParallel(net)\n",
        "        net.to(device)\n",
        "\n",
        "        # net params\n",
        "        criterion = SF.mse_count_loss(correct_rate=config['correct_rate'], incorrect_rate=config['incorrect_rate'])\n",
        "        optimizer, scheduler = optim_func(net, config)\n",
        "\n",
        "        # early stopping condition\n",
        "        if config['early_stopping']:\n",
        "            early_stopping = EarlyStopping_acc(patience=config['patience'], verbose=True, path=model_name)\n",
        "            early_stopping.early_stop = False\n",
        "            early_stopping.best_score = None\n",
        "\n",
        "        # load data\n",
        "        trainset, testset = load_data(config)\n",
        "        config['dataset_length'] = len(trainset)\n",
        "        trainloader = DataLoader(trainset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
        "        testloader = DataLoader(testset, batch_size=int(config[\"batch_size\"]), shuffle=False)\n",
        "\n",
        "        print(f\"=======Trial: {trial}=======\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            # train\n",
        "            start_time = time.time()\n",
        "            loss_list, lr_list = train(config, net, epoch, trainloader, testloader, criterion, optimizer, scheduler, device)\n",
        "            epoch_time = time.time() - start_time\n",
        "\n",
        "            # test\n",
        "            test_acc = test_accuracy(config, net, testloader, device)\n",
        "            print(f'Epoch: {epoch} \\tTest Accuracy: {test_acc}')\n",
        "\n",
        "            if config['df_lr']:\n",
        "                df_lr = pd.concat([df_lr, pd.DataFrame(lr_list)])\n",
        "            df_train_loss = pd.concat([df_train_loss, pd.DataFrame(loss_list)])\n",
        "            test_data = pd.DataFrame([[epoch, test_acc, epoch_time]], columns = ['epoch', 'test_acc', 'train_time'])\n",
        "            df_test_acc = pd.concat([df_test_acc, test_data])\n",
        "\n",
        "            if SAVE_CSV:\n",
        "                df_train_loss.to_csv('loss_' + csv_name, index=False)\n",
        "                df_test_acc.to_csv('acc_' + csv_name, index=False)\n",
        "                if config['df_lr']:\n",
        "                    df_lr.to_csv('lr_' + csv_name, index=False)\n",
        "\n",
        "            if config['early_stopping']:\n",
        "                early_stopping(test_acc, net)\n",
        "\n",
        "                if early_stopping.early_stop:\n",
        "                    print(\"Early stopping\")\n",
        "                    early_stopping.early_stop = False\n",
        "                    early_stopping.best_score = None\n",
        "                    break\n",
        "\n",
        "            if SAVE_MODEL and not config['early_stopping']:\n",
        "                torch.save(net.state_dict(), model_name)\n",
        "\n",
        "            #for param in net.parameters():\n",
        "            #    print(param)\n",
        "        # net.load_state_dict(torch.load(model_name))\n"
      ],
      "metadata": {
        "id": "S7d0Bj7qMjyp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# snntorch\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NetConv(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.thr1 = config['threshold1']\n",
        "        self.thr2 = config['threshold2']\n",
        "        self.thr3 = config['threshold3']\n",
        "        slope = config['slope']\n",
        "        beta = config['beta']\n",
        "        self.num_steps = config['num_steps']\n",
        "        self.batch_norm = config['batch_norm']\n",
        "        p1 = config['dropout1']\n",
        "        self.binarize = config['binarize']\n",
        "        self.binarize_input = config['binarize_input']\n",
        "\n",
        "        spike_grad = surrogate.fast_sigmoid(slope)\n",
        "        # Initialize layers with spike operator\n",
        "        self.bconv1 = BinaryConv2d(1, 16, 5, bias=False)\n",
        "        self.conv1 = nn.Conv2d(1, 16, 5, bias=False)\n",
        "        self.conv1_bn = nn.BatchNorm2d(16)\n",
        "        self.lif1 = snn.Leaky(beta, threshold=self.thr1, spike_grad=spike_grad)\n",
        "        self.bconv2 = BinaryConv2d(16, 64, 5, bias=False)\n",
        "        self.conv2 = nn.Conv2d(16, 64, 5, bias=False)\n",
        "        self.conv2_bn = nn.BatchNorm2d(64)\n",
        "        self.lif2 = snn.Leaky(beta, threshold=self.thr2, spike_grad=spike_grad)\n",
        "        self.bfc1 = BinaryLinear(64 * 4 * 4, 10)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 10)\n",
        "        self.lif3 = snn.Leaky(beta, threshold=self.thr3, spike_grad=spike_grad)\n",
        "        self.dropout = nn.Dropout(p1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states and outputs at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        # Binarized\n",
        "        if self.binarize:\n",
        "\n",
        "            for step in range(self.num_steps):\n",
        "\n",
        "                if self.binarize_input:\n",
        "                    x = binarize_activations(x)\n",
        "                cur1 = F.avg_pool2d(self.bconv1(x), 2)\n",
        "                if self.batch_norm:\n",
        "                    cur1 = self.conv1_bn(cur1)\n",
        "                spk1, mem1 = self.lif1(cur1, mem1)\n",
        "                cur2 = F.avg_pool2d(self.bconv2(spk1), 2)\n",
        "                if self.batch_norm:\n",
        "                    cur2 = self.conv2_bn(cur2)\n",
        "                spk2, mem2 = self.lif2(cur2, mem2)\n",
        "                cur3 = self.dropout(self.bfc1(spk2.flatten(1)))\n",
        "                spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "                spk3_rec.append(spk3)\n",
        "                mem3_rec.append(mem3)\n",
        "\n",
        "            return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "        # Full Precision\n",
        "        else:\n",
        "\n",
        "            for step in range(self.num_steps):\n",
        "\n",
        "                cur1 = F.avg_pool2d(self.conv1(x), 2)\n",
        "                if self.batch_norm:\n",
        "                    cur1 = self.conv1_bn(cur1)\n",
        "                spk1, mem1 = self.lif1(cur1, mem1)\n",
        "                cur2 = F.avg_pool2d(self.conv2(spk1), 2)\n",
        "                if self.batch_norm:\n",
        "                    cur2 = self.conv2_bn(cur2)\n",
        "                spk2, mem2 = self.lif2(cur2, mem2)\n",
        "                cur3 = self.dropout(self.fc1(spk2.flatten(1)))\n",
        "                spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "                spk3_rec.append(spk3)\n",
        "                mem3_rec.append(mem3)\n",
        "\n",
        "            return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "class NetFC(nn.Module):\n",
        "    def __init__(self, config, neurons=[256, 128], sparsity=[0.0, 0.0]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.thr1 = config['threshold1']\n",
        "        self.thr2 = config['threshold2']\n",
        "        self.thr3 = config['threshold3']\n",
        "        slope = config['slope']\n",
        "        beta = config['beta']\n",
        "        self.num_steps = config['num_steps']\n",
        "        self.batch_norm = config['batch_norm']\n",
        "        p1 = config['dropout1']\n",
        "        self.binarize = config['binarize']\n",
        "        self.binarize_input = config['binarize_input']\n",
        "\n",
        "        spike_grad = surrogate.fast_sigmoid(slope)\n",
        "        # Initialize layers with spike operator\n",
        "        self.bfc1 = SparseBinaryLinear(MNIST_INPUT_RESOLUTION * MNIST_INPUT_RESOLUTION, neurons[0], sparsity[0], bias=False)\n",
        "        self.fc1 = nn.Linear(MNIST_INPUT_RESOLUTION * MNIST_INPUT_RESOLUTION, neurons[0], bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(neurons[0])\n",
        "        self.lif1 = snn.Leaky(beta, threshold=self.thr1, spike_grad=spike_grad)\n",
        "\n",
        "        self.bfc2 = SparseBinaryLinear(neurons[0], neurons[1], sparsity[1], bias=False)\n",
        "        self.fc2 = nn.Linear(neurons[0], neurons[1], bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(neurons[1])\n",
        "        self.lif2 = snn.Leaky(beta, threshold=self.thr2, spike_grad=spike_grad)\n",
        "\n",
        "        self.bfc3 = BinaryLinear(neurons[1], 10, bias=False)\n",
        "        self.fc3 = nn.Linear(neurons[1], 10, bias=False)\n",
        "        self.lif3 = snn.Leaky(beta, threshold=self.thr3, spike_grad=spike_grad)\n",
        "        self.dropout = nn.Dropout(p1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states and outputs at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        # Binarized\n",
        "        if self.binarize:\n",
        "            for step in range(self.num_steps):\n",
        "\n",
        "                x = x.flatten(1)\n",
        "                if self.binarize_input:\n",
        "                    x = binarize_activations(x)\n",
        "                cur1 = self.bfc1(x)\n",
        "                if self.batch_norm:\n",
        "                    cur1 = self.bn1(cur1)\n",
        "                spk1, mem1 = self.lif1(cur1, mem1)\n",
        "                cur2 = self.bfc2(spk1)\n",
        "                if self.batch_norm:\n",
        "                    cur2 = self.bn2(cur2)\n",
        "                spk2, mem2 = self.lif2(cur2, mem2)\n",
        "                cur3 = self.dropout(self.bfc3(spk2))\n",
        "                spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "                spk3_rec.append(spk3)\n",
        "                mem3_rec.append(mem3)\n",
        "\n",
        "            return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "        # Full Precision\n",
        "        else:\n",
        "\n",
        "            for step in range(self.num_steps):\n",
        "\n",
        "                cur1 = self.fc1(x.flatten(1))\n",
        "                if self.batch_norm:\n",
        "                    cur1 = self.bn1(cur1)\n",
        "                spk1, mem1 = self.lif1(cur1, mem1)\n",
        "                cur2 = self.fc2(spk1)\n",
        "                if self.batch_norm:\n",
        "                    cur2 = self.bn2(cur2)\n",
        "                spk2, mem2 = self.lif2(cur2, mem2)\n",
        "                cur3 = self.dropout(self.fc3(spk2))\n",
        "                spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "                spk3_rec.append(spk3)\n",
        "                mem3_rec.append(mem3)\n",
        "\n",
        "            return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "class NetFC_FirstConv(nn.Module):\n",
        "    def __init__(self, config, neurons=[16, 256]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.thr1 = config['threshold1']\n",
        "        self.thr2 = config['threshold2']\n",
        "        self.thr3 = config['threshold3']\n",
        "        slope = config['slope']\n",
        "        beta = config['beta']\n",
        "        self.num_steps = config['num_steps']\n",
        "        self.batch_norm = config['batch_norm']\n",
        "        p1 = config['dropout1']\n",
        "        self.binarize = config['binarize']\n",
        "        self.binarize_input = config['binarize_input']\n",
        "\n",
        "        spike_grad = surrogate.fast_sigmoid(slope)\n",
        "        # Initialize layers with spike operator\n",
        "        self.bconv1 = BinaryConv2d(1, neurons[0], 5, bias=False)\n",
        "        self.conv1 = nn.Conv2d(1, neurons[0], 5, bias=False)\n",
        "        self.conv1_bn = nn.BatchNorm2d(neurons[0])\n",
        "        self.lif1 = snn.Leaky(beta, threshold=self.thr1, spike_grad=spike_grad)\n",
        "\n",
        "        n = ((28-(5//2))/2)**2\n",
        "        print(n)\n",
        "        self.bfc2 = BinaryLinear(n, neurons[1])\n",
        "        self.fc2 = nn.Linear(n, neurons[1])\n",
        "        self.bn2 = nn.BatchNorm1d(neurons[1])\n",
        "        self.lif2 = snn.Leaky(beta, threshold=self.thr2, spike_grad=spike_grad)\n",
        "\n",
        "        self.bfc3 = BinaryLinear(neurons[1], 10)\n",
        "        self.fc3 = nn.Linear(neurons[1], 10)\n",
        "        self.lif3 = snn.Leaky(beta, threshold=self.thr3, spike_grad=spike_grad)\n",
        "        self.dropout = nn.Dropout(p1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states and outputs at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        # Binarized\n",
        "        if self.binarize:\n",
        "\n",
        "            for step in range(self.num_steps):\n",
        "\n",
        "                if self.binarize_input:\n",
        "                    x = binarize_activations(x)\n",
        "                cur1 = F.avg_pool2d(self.bconv1(x), 2)\n",
        "                if self.batch_norm:\n",
        "                    cur1 = self.conv1_bn(cur1)\n",
        "                spk1, mem1 = self.lif1(cur1, mem1)\n",
        "                spk2, mem2 = self.lif2(self.bfc2(spk1.flatten(1)), mem2)\n",
        "                if self.batch_norm:\n",
        "                    spk2 = self.bn2(spk2)\n",
        "                cur3 = self.dropout(self.bfc3(spk2))\n",
        "                spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "                spk3_rec.append(spk3)\n",
        "                mem3_rec.append(mem3)\n",
        "\n",
        "            return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "        # Full Precision\n",
        "        else:\n",
        "\n",
        "            for step in range(self.num_steps):\n",
        "\n",
        "                cur1 = F.avg_pool2d(self.conv1(x), 2)\n",
        "                if self.batch_norm:\n",
        "                    cur1 = self.conv1_bn(cur1)\n",
        "                spk1, mem1 = self.lif1(cur1, mem1)\n",
        "                spk2, mem2 = self.lif2(self.fc2(spk1.flatten(1)), mem2)\n",
        "                if self.batch_norm:\n",
        "                    spk2 = self.bn2(spk2)\n",
        "                cur3 = self.dropout(self.fc3(spk2))\n",
        "                spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "                spk3_rec.append(spk3)\n",
        "                mem3_rec.append(mem3)\n",
        "\n",
        "            return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)"
      ],
      "metadata": {
        "id": "U_IRbNKuMgho"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = config\n",
        "cfg['model'] = 'NetFC'\n",
        "run(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "ilgG9-7Oda39",
        "outputId": "85aba8e9-7df1-489a-e08c-5df3b9817f85"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'NetFC', 'exp_name': 'mnist_tha', 'num_trials': 5, 'num_epochs': 500, 'binarize': True, 'data_dir': '~/data/mnist', 'batch_size': 128, 'seed': 0, 'num_workers': 0, 'save_csv': True, 'save_model': True, 'early_stopping': True, 'patience': 100, 'grad_clip': False, 'weight_clip': False, 'batch_norm': True, 'dropout1': 0.02856, 'beta': 0.99, 'lr': 0.00997, 'slope': 10.22, 'threshold1': 11.666, 'alpha_thr1': 0.024, 'thr_final1': 4.317, 'threshold2': 14.105, 'alpha_thr2': 0.119, 'thr_final2': 16.29, 'threshold3': 0.6656, 'alpha_thr3': 0.0011, 'thr_final3': 3.496, 'num_steps': 100, 'correct_rate': 0.8, 'incorrect_rate': 0.2, 'betas': (0.9, 0.999), 't_0': 4688, 'eta_min': 0, 'df_lr': True}\n",
            "NetFC(\n",
            "  (bfc1): SparseBinaryLinear(in_features=256, out_features=256, bias=False)\n",
            "  (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lif1): Leaky()\n",
            "  (bfc2): SparseBinaryLinear(in_features=256, out_features=128, bias=False)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=False)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lif2): Leaky()\n",
            "  (bfc3): BinaryLinear(in_features=128, out_features=10, bias=False)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=False)\n",
            "  (lif3): Leaky()\n",
            "  (dropout): Dropout(p=0.02856, inplace=False)\n",
            ")\n",
            "=======Trial: 0=======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.7818: 100%|██████████| 469/469 [03:02<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \tTest Accuracy: 94.05\n",
            "Test acc increased (0.000000 --> 94.050000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.7076:  51%|█████     | 238/469 [01:29<01:26,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9836685b21b3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'NetFC'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-24fc7e61737e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-e3ed6f056ebd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, net, epoch, trainloader, testloader, criterion, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk_rec2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss_current\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#binarized inputs\n",
        "cfg = config\n",
        "cfg['model'] = 'NetFC'\n",
        "run(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "INPErVUdnw5s",
        "outputId": "9f60ea13-72ca-449d-c750-fbc90ad10d0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'NetFC', 'exp_name': 'mnist_tha', 'num_trials': 5, 'num_epochs': 500, 'binarize': True, 'data_dir': '~/data/mnist', 'batch_size': 128, 'seed': 0, 'num_workers': 0, 'save_csv': True, 'save_model': True, 'early_stopping': True, 'patience': 100, 'grad_clip': False, 'weight_clip': False, 'batch_norm': True, 'dropout1': 0.02856, 'beta': 0.99, 'lr': 0.00997, 'slope': 10.22, 'threshold1': 11.666, 'alpha_thr1': 0.024, 'thr_final1': 4.317, 'threshold2': 14.105, 'alpha_thr2': 0.119, 'thr_final2': 16.29, 'threshold3': 0.6656, 'alpha_thr3': 0.0011, 'thr_final3': 3.496, 'num_steps': 100, 'correct_rate': 0.8, 'incorrect_rate': 0.2, 'betas': (0.9, 0.999), 't_0': 4688, 'eta_min': 0, 'df_lr': True}\n",
            "NetFC(\n",
            "  (bfc1): SparseBinaryLinear(in_features=256, out_features=256, bias=False)\n",
            "  (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lif1): Leaky()\n",
            "  (bfc2): SparseBinaryLinear(in_features=256, out_features=128, bias=False)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=False)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lif2): Leaky()\n",
            "  (bfc3): BinaryLinear(in_features=128, out_features=10, bias=False)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=False)\n",
            "  (lif3): Leaky()\n",
            "  (dropout): Dropout(p=0.02856, inplace=False)\n",
            ")\n",
            "=======Trial: 0=======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.8944: 100%|██████████| 469/469 [02:55<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \tTest Accuracy: 91.56\n",
            "Test acc increased (0.000000 --> 91.560000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.8133: 100%|██████████| 469/469 [02:55<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTest Accuracy: 93.5\n",
            "Test acc increased (91.560000 --> 93.500000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.7441:  27%|██▋       | 125/469 [00:46<02:08,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-62141dcf62a4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'NetFC'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-24fc7e61737e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e3ed6f056ebd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, net, epoch, trainloader, testloader, criterion, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mspk_rec2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk_rec2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1db99282087d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mcur2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mspk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mcur3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mspk3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/_neurons/leaky.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_state_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_quant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/_neurons/leaky.py\u001b[0m in \u001b[0;36m_build_state_function\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_state_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_mechanism_val\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# reset by subtraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             state_fn = self._base_state_function(\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/_neurons/leaky.py\u001b[0m in \u001b[0;36m_base_state_function\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_base_state_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mbase_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = config\n",
        "cfg['model'] = 'NetFC(config, sparsity=[0.5, 0.5])'\n",
        "run(cfg)"
      ],
      "metadata": {
        "id": "Bav7tGbu-lI7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8a4169e-5621-4e33-b7a9-00a5d219f3fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model': 'NetFC(config, sparsity=0.5)', 'exp_name': 'mnist_tha', 'num_trials': 5, 'num_epochs': 500, 'binarize': True, 'data_dir': '~/data/mnist', 'batch_size': 128, 'seed': 0, 'num_workers': 0, 'save_csv': True, 'save_model': True, 'early_stopping': True, 'patience': 100, 'grad_clip': False, 'weight_clip': False, 'batch_norm': True, 'dropout1': 0.02856, 'beta': 0.99, 'lr': 0.00997, 'slope': 10.22, 'threshold1': 11.666, 'alpha_thr1': 0.024, 'thr_final1': 15.983, 'threshold2': 14.105, 'alpha_thr2': 0.119, 'thr_final2': 30.395, 'threshold3': 0.6656, 'alpha_thr3': 0.0011, 'thr_final3': 4.1616, 'num_steps': 100, 'correct_rate': 0.8, 'incorrect_rate': 0.2, 'betas': (0.9, 0.999), 't_0': 4688, 'eta_min': 0, 'df_lr': True, 'dataset_length': 60000}\n",
            "NetFC(\n",
            "  (bfc1): SparseBinaryLinear(in_features=256, out_features=256, bias=False)\n",
            "  (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lif1): Leaky()\n",
            "  (bfc2): SparseBinaryLinear(in_features=256, out_features=128, bias=False)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=False)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lif2): Leaky()\n",
            "  (bfc3): BinaryLinear(in_features=128, out_features=10, bias=False)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=False)\n",
            "  (lif3): Leaky()\n",
            "  (dropout): Dropout(p=0.02856, inplace=False)\n",
            ")\n",
            "=======Trial: 0=======\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 1.0576: 100%|██████████| 469/469 [02:57<00:00,  2.64it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tTest Accuracy: 89.58\n",
            "Test acc increased (0.000000 --> 89.580000).  Saving model ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.9438: 100%|██████████| 469/469 [02:57<00:00,  2.64it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTest Accuracy: 90.51\n",
            "Test acc increased (89.580000 --> 90.510000).  Saving model ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.9071: 100%|██████████| 469/469 [02:57<00:00,  2.64it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 \tTest Accuracy: 91.0\n",
            "Test acc increased (90.510000 --> 91.000000).  Saving model ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.8651: 100%|██████████| 469/469 [02:57<00:00,  2.64it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 \tTest Accuracy: 91.85\n",
            "Test acc increased (91.000000 --> 91.850000).  Saving model ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.8304: 100%|██████████| 469/469 [02:53<00:00,  2.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 \tTest Accuracy: 92.0\n",
            "Test acc increased (91.850000 --> 92.000000).  Saving model ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.8051: 100%|██████████| 469/469 [02:52<00:00,  2.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 \tTest Accuracy: 92.3\n",
            "Test acc increased (92.000000 --> 92.300000).  Saving model ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.7615: 100%|██████████| 469/469 [02:50<00:00,  2.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6 \tTest Accuracy: 92.17\n",
            "EarlyStopping counter: 1 out of 100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.7586: 100%|██████████| 469/469 [02:49<00:00,  2.77it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7 \tTest Accuracy: 92.73\n",
            "Test acc increased (92.300000 --> 92.730000).  Saving model ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.7067: 100%|██████████| 469/469 [02:47<00:00,  2.80it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8 \tTest Accuracy: 92.95\n",
            "Test acc increased (92.730000 --> 92.950000).  Saving model ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 0.6712: 100%|██████████| 469/469 [02:44<00:00,  2.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9 \tTest Accuracy: 92.91\n",
            "EarlyStopping counter: 1 out of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.7154:  30%|███       | 141/469 [00:50<01:58,  2.78it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b909bb6cfd8a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'NetFC(config, sparsity=0.5)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-24fc7e61737e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e3ed6f056ebd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, net, epoch, trainloader, testloader, criterion, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mspk_rec2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk_rec2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1db99282087d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mspk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mcur3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0mspk3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mspk3_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/_neurons/leaky.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_state_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_quant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/_neurons/leaky.py\u001b[0m in \u001b[0;36m_build_state_function\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_state_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_mechanism_val\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# reset by subtraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             state_fn = self._base_state_function(\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/_neurons/leaky.py\u001b[0m in \u001b[0;36m_base_state_function\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_base_state_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mbase_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = config\n",
        "cfg['model'] = 'NetFC(config, sparsity=[0.9, 0.9])'\n",
        "run(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAdG8QcKymlc",
        "outputId": "7328ec2f-48b4-48f0-92c1-36eabd81818d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'NetFC(config, sparsity=0.9)', 'exp_name': 'mnist_tha', 'num_trials': 5, 'num_epochs': 10, 'binarize': True, 'binarize_input': True, 'data_dir': '~/data/mnist', 'batch_size': 128, 'seed': 0, 'num_workers': 0, 'save_csv': True, 'save_model': True, 'early_stopping': True, 'patience': 100, 'grad_clip': False, 'weight_clip': False, 'batch_norm': True, 'dropout1': 0.02856, 'beta': 0.99, 'lr': 0.00997, 'slope': 10.22, 'threshold1': 11.666, 'alpha_thr1': 0.024, 'thr_final1': 4.317, 'threshold2': 14.105, 'alpha_thr2': 0.119, 'thr_final2': 16.29, 'threshold3': 0.6656, 'alpha_thr3': 0.0011, 'thr_final3': 3.496, 'num_steps': 100, 'correct_rate': 0.8, 'incorrect_rate': 0.2, 'betas': (0.9, 0.999), 't_0': 4688, 'eta_min': 0, 'df_lr': True}\n",
            "tensor(0.1001)\n",
            "tensor(0.1024)\n",
            "NetFC(\n",
            "  (bfc1): SparseBinaryLinear(in_features=256, out_features=256, bias=False)\n",
            "  (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lif1): Leaky()\n",
            "  (bfc2): SparseBinaryLinear(in_features=256, out_features=128, bias=False)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=False)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lif2): Leaky()\n",
            "  (bfc3): BinaryLinear(in_features=128, out_features=10, bias=False)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=False)\n",
            "  (lif3): Leaky()\n",
            "  (dropout): Dropout(p=0.02856, inplace=False)\n",
            ")\n",
            "=======Trial: 0=======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.6109: 100%|██████████| 469/469 [02:47<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \tTest Accuracy: 85.25\n",
            "Test acc increased (0.000000 --> 85.250000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.4953: 100%|██████████| 469/469 [02:46<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTest Accuracy: 84.37\n",
            "EarlyStopping counter: 1 out of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.4063: 100%|██████████| 469/469 [02:48<00:00,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 \tTest Accuracy: 85.62\n",
            "Test acc increased (85.250000 --> 85.620000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.3945: 100%|██████████| 469/469 [02:48<00:00,  2.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 \tTest Accuracy: 86.7\n",
            "Test acc increased (85.620000 --> 86.700000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.4162: 100%|██████████| 469/469 [02:46<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 \tTest Accuracy: 87.12\n",
            "Test acc increased (86.700000 --> 87.120000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.3976: 100%|██████████| 469/469 [02:46<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 \tTest Accuracy: 87.33\n",
            "Test acc increased (87.120000 --> 87.330000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.2937: 100%|██████████| 469/469 [02:48<00:00,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6 \tTest Accuracy: 87.65\n",
            "Test acc increased (87.330000 --> 87.650000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.3561:  95%|█████████▍| 445/469 [02:38<00:08,  2.76it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = config\n",
        "cfg['model'] = 'NetFC(config, [128, 128])'\n",
        "run(cfg)"
      ],
      "metadata": {
        "id": "15U4jQ5WAj2O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}